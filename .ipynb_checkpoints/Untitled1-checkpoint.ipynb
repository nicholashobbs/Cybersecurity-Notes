{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Resume\n",
    "\n",
    "\n",
    "## CV Project\n",
    "\n",
    "### Background\n",
    "\n",
    "* This project involves trying to make sense of a dataset from ILRI in Kenya which has 120,000 geotagged cell phone pictures which are associated with survey data.\n",
    "* The survey asked pastoralists to rate the carrying capacity of the land by asking how many cows could be fed by the available forage within 20 steps in every direction for one day.\n",
    "* The survey then asked participants whether there were grass, shrubs or trees in the area and whether they had green, brown or no leaves. It also asked whether there were animals grazing, and how far away the nearest source of water was.\n",
    "\n",
    "### Issues\n",
    "\n",
    "* There are a number of issues with the data collected and the survey design. The images are poor quality and seem to have been processed somehow, although it's not clear how. Colors seem to be blended together in a strange way, as if the images have been compressed.\n",
    "* Some of the pictures are of people, villages, or people holding up printed pictures. Some pictures are clearly geotagged wrong, indicating that they may have been uploaded from a different place than where they were taken.\n",
    "* The way that herders were asked about carrying capacity is problematic. Participants were asked about an area within a 20 step radius, but the pictures only show one direction which may or may not be consistent with the carrying capacity rating.\n",
    "* Some of the survey data is unintelligible. For example, there are certain rows which have a value 3 3 1 1 2 2 in a column where the possible values are 1 2 and 3. The survey app is based on OpenDataKit Collect, and it seems like these responses mean that someone checked and unchecked an option, resulting in 2 clicks. For now, I have just discarded these rows for the few experiments I have run, because there are not very many instances.\n",
    "\n",
    "\n",
    "### Methodology\n",
    "\n",
    "* The goal is to use survey data along with MODIS satellite data to try to find techniques which get close to the results of the NDVI (Normalized Difference Vegetation Index).\n",
    "* The NDVI works by measuring the ratio of visible light(0.4-0.7 $\\mu$m), which is strongly absorbed by chlorophyl for photosynthesis, vs near infrared light (0.7-1.1 $\\mu$m), which is strongly reflected by the cell structure of leaves. $NDVI = \\frac{NIR - Red}{NIR + Red}$\n",
    "* So I am trying to get inferences from the survey questions and images themselves, and then test those against the results of the MODIS data and the farmer's estimation of the land's carrying capacity.\n",
    "* When the researchers originally started having issues with the dataset, they tried to get mechanical turks to rate the carrying capacity. This is another questionably accurate source of data but another thing to use for truth checking and to come up with ideas.\n",
    "* To start I created an efficient way to preview a few random images from whatever subset of the data I select without downloading all 120,000.\n",
    "* Then I created subsets of the data based on seemingly consistent and inconsistent answers. I created a 'score' for answers which suggest higher carrying capacity, and another score which is for variables which suggest lower carrying capacity. for example green grass and dense grass, high carrying capacity and lots of animals, etc. Images with inconsistent tags do turn out to be more likely junk data, but most images have some \"negative\" and some \"positive\" indicators, meaning that this is only good for identifying some certain bad images.\n",
    "* We initially tried using a basic tensorflow example to predict carrying capacity, which took a long time and didnt work at all. Then we tried to use inception just to see whether it could recognize trees accurately. This didn't work very well.\n",
    "* Finally, I've started trying with some cleaned up subsets of the data to run randomforest classifiers and then inspect which variables most impact carryingcapacity. This has given us more information than anything so far, so I'm going to keep going down this path by trying different algorithms and other ways to use the survey data to infer what is in the pictures or what the satellite says.\n",
    "\n",
    "## Cyber Insurance Presentation\n",
    "\n",
    "1. People need to do the cheapest and easiest thing first because a lot of attorneys have no time or desire to learn about cybersecurity. This starts with basically awareness: having security policies, permissions, knowing what devices are on the network, having a clean desk, doing data inventory, setting permissions, etc.\n",
    "2. GDPR AND CalCPA are coming, and what this means\n",
    "3. Cyberinsurance and how to shop for it to help protect you against regulation\n",
    "4. Further resources, checklists and where to go if you need more help.\n",
    "\n",
    "\n",
    "\n",
    "## Freelance Work\n",
    "\n",
    "## SIC\n",
    "\n",
    "## Evolve\n",
    "\n",
    "## College\n",
    "\n",
    "# Questions for CyberCube\n",
    "\n",
    "1. What type of role exactly would this be? Cyber, Data, Risk, or a combination?\n",
    "\n",
    "2. What should I study for the next interview?\n",
    "\n",
    "3. Can I ask you some questions I had for this upcoming presentation? Would you prefer to do that a different time?\n",
    "\n",
    "4. Where does an organization's security posture come in to making Insurance decisions about the client?\n",
    "\n",
    "Questions from the At-Bay Calculator\n",
    "\n",
    "Customer & Employee Records or just Employee\n",
    "* How many people were affected?\n",
    "* What types of records? PII? Credit Card? Health?\n",
    "* How were records breached? Error, Leak, Device Theft, Hack\n",
    "* Do you store the mailing addresses for breached records?\n",
    "* Have you publicly disclosed another breach in the last 24 months?\n",
    "* How would you estimate the level of complexity of your network?\n",
    "* How big of a news story would this breach be? no news, regional news, national news\n",
    "* How would you estimate your security controls compared to industry best practices? Average, above, or below\n",
    "* Are you based out of California?\n",
    "\n",
    "Costs:\n",
    "* Breach Coach\n",
    "* Forensics\n",
    "* Crisis Management\n",
    "* Notification\n",
    "* Call Center\n",
    "* Credit Monitoring\n",
    "* PCI Fines and Assessments\n",
    "* Regulatory Fines and Defense\n",
    "* Class Action Settlements and Defense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
